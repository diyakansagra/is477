{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "961bb6bf-a6d1-4c62-b41d-dfa7b91958eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: recordlinkage in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (0.16)\n",
      "Requirement already satisfied: rapidfuzz in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (3.14.3)\n",
      "Requirement already satisfied: jellyfish>=1 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from recordlinkage) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.13 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from recordlinkage) (1.26.4)\n",
      "Requirement already satisfied: pandas<3,>=1 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from recordlinkage) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from recordlinkage) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from recordlinkage) (1.5.1)\n",
      "Requirement already satisfied: joblib in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from recordlinkage) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from pandas<3,>=1->recordlinkage) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from pandas<3,>=1->recordlinkage) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from pandas<3,>=1->recordlinkage) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from scikit-learn>=1->recordlinkage) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1->recordlinkage) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install recordlinkage rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e91a685-8017-40c8-8915-397f163dcb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB columns: ['title', 'director', 'release_year', 'genre', 'rating', 'metascore', 'runtime_in_minutes', 'gross_in_millions', 'title_norm', 'genre_norm']\n",
      "TMDB columns: ['budget_in_millions', 'popularity', 'revenue_in_millions', 'runtime_in_minutes', 'title', 'vote_average', 'vote_count', 'genre', 'release_year', 'title_norm', 'genre_norm']\n",
      "Integration done. Outputs:\n",
      " - integration_output/merged_movies.csv\n",
      " - integration_output/merge_log.json\n",
      "{\n",
      "  \"imdb_total_rows\": 705,\n",
      "  \"tmdb_total_rows\": 3177,\n",
      "  \"exact_match_count\": 379,\n",
      "  \"left_only_before_fuzzy\": 326,\n",
      "  \"fuzzy_match_count\": 8,\n",
      "  \"final_merged_rows\": 705,\n",
      "  \"exact_matches_saved\": 379,\n",
      "  \"fuzzy_matches_saved\": 8,\n",
      "  \"unmatched_imdb_saved\": 318\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# First I will try to import recordlinkage, if not, it the funtion should fallback to rapidfuzz\n",
    "USE_RECORDLINKAGE = False\n",
    "try:\n",
    "    import recordlinkage as rl\n",
    "    USE_RECORDLINKAGE = True\n",
    "except Exception:\n",
    "    try:\n",
    "        from rapidfuzz import fuzz, process\n",
    "    except Exception:\n",
    "        raise RuntimeError(\"Please install either 'recordlinkage' or 'rapidfuzz' to run fuzzy linking.\")\n",
    "\n",
    "IMDB_PATH = \"imdb_cleaned.csv\"\n",
    "TMDB_PATH = \"tmdb_cleaned.csv\"\n",
    "\n",
    "OUTPUT_DIR = \"integration_output\"\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "MERGED_CSV = Path(OUTPUT_DIR) / \"merged_movies.csv\"\n",
    "LOG_JSON = Path(OUTPUT_DIR) / \"merge_log.json\"\n",
    "\n",
    "# These will be the Mapping rules (which columns to keep & which hold precedence/priority over other columns)\n",
    "# I will keep all imdb columns, and add tmdb-specific columns that imdb does not have to aid in our project endeavors.\n",
    "IMDB_KEEP = [\"title\", \"director\", \"release_year\", \"genre\", \"rating\", \"metascore\", \"runtime_in_minutes\", \"gross_in_millions\"]\n",
    "TMDB_KEEP = [\"budget_in_millions\", \"popularity\", \"revenue_in_millions\", \"runtime_in_minutes\", \"vote_average\", \"vote_count\", \"genre\", \"release_year\"]\n",
    "\n",
    "# Precedence rules I will enforce in the python code below:\n",
    "# If imdb and tmdb both provide a value for a shared column:\n",
    "# - For ratings: prefer IMDb 'rating' as they are generally used more often, and also preserve TMDB 'vote_average' as separate column\n",
    "# - For runtime/release_year/genre: if they disagree, keep both columns (runtime_imdb, runtime_tmdb) so downstream analysis can decide (I may also use the average of both for a better representation, during analysis i will revisit this)\n",
    "# - For budget/revenue/popularity prefer TMDB (IMDb often lacks budget/revenue for older movies, TMDB is more up-to-date)\n",
    "# - Title matching uses normalized title + release_year. If release_year missing, block less strictly but prefer exact title.\n",
    "\n",
    "def load_and_preview(imdb_path=IMDB_PATH, tmdb_path=TMDB_PATH):\n",
    "    imdb = pd.read_csv(imdb_path).convert_dtypes()\n",
    "    tmdb = pd.read_csv(tmdb_path).convert_dtypes()\n",
    "\n",
    "    # Remove any duplicate columns, this ensures a clean schema before merging, as I ran into issues when columns were named the same\n",
    "    imdb = imdb.loc[:, ~imdb.columns.duplicated()]\n",
    "    tmdb = tmdb.loc[:, ~tmdb.columns.duplicated()]\n",
    "\n",
    "    return imdb, tmdb\n",
    "\n",
    "def normalize_columns(imdb, tmdb):\n",
    "    imdb = imdb.rename(columns=lambda c: c.strip())\n",
    "    tmdb = tmdb.rename(columns=lambda c: c.strip())\n",
    "\n",
    "    imdb = imdb.loc[:, ~imdb.columns.duplicated()]\n",
    "    tmdb = tmdb.loc[:, ~tmdb.columns.duplicated()]\n",
    "\n",
    "    imdb = make_unique_cols(imdb)\n",
    "    tmdb = make_unique_cols(tmdb)\n",
    "\n",
    "    # Standardize column names (relitively simple since my cleaning addressed these)\n",
    "    imdb = imdb.rename(columns={\n",
    "        \"title\": \"title\",\n",
    "        \"director\": \"director\",\n",
    "        \"release_year\": \"release_year\",\n",
    "        \"genre\": \"genre\",\n",
    "        \"rating\": \"rating\",\n",
    "        \"metascore\": \"metascore\",\n",
    "        \"runtime_in_minutes\": \"runtime_in_minutes\",\n",
    "        \"gross_in_millions\": \"gross_in_millions\"\n",
    "    })\n",
    "\n",
    "    tmdb = tmdb.rename(columns={\n",
    "        \"budget_in_millions\": \"budget_in_millions\",\n",
    "        \"popularity\": \"popularity\",\n",
    "        \"revenue_in_millions\": \"revenue_in_millions\",\n",
    "        \"runtime_in_minutes\": \"runtime_in_minutes\",\n",
    "        \"title\": \"title\",\n",
    "        \"vote_average\": \"vote_average\",\n",
    "        \"vote_count\": \"vote_count\",\n",
    "        \"genre\": \"genre\",\n",
    "        \"release_year\": \"release_year\"\n",
    "    })\n",
    "\n",
    "    # Normalize types: ensure numeric columns are numeric and ensure years are read in as integers\n",
    "    for col in [\"release_year\", \"runtime_in_minutes\"]:\n",
    "        if col in imdb.columns:\n",
    "            imdb[col] = pd.to_numeric(imdb[col], errors=\"coerce\").astype(\"Int64\")\n",
    "        if col in tmdb.columns:\n",
    "            tmdb[col] = pd.to_numeric(tmdb[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    for col in [\"rating\", \"metascore\", \"gross_in_millions\", \"budget_in_millions\", \"popularity\", \"revenue_in_millions\", \"vote_average\", \"vote_count\"]:\n",
    "        if col in imdb.columns:\n",
    "            imdb[col] = pd.to_numeric(imdb[col], errors=\"coerce\")\n",
    "        if col in tmdb.columns:\n",
    "            tmdb[col] = pd.to_numeric(tmdb[col], errors=\"coerce\")\n",
    "\n",
    "    # Here I Normalize title strings by using strip, lower, and remove extra whitespace\n",
    "    def norm_title(s):\n",
    "        if pd.isna(s):\n",
    "            return \"\"\n",
    "        return \" \".join(str(s).strip().lower().split())\n",
    "\n",
    "    imdb[\"title_norm\"] = imdb[\"title\"].apply(norm_title)\n",
    "    tmdb[\"title_norm\"] = tmdb[\"title\"].apply(norm_title)\n",
    "\n",
    "    # Now I'll also create genre_norm for matching (lower & strip)\n",
    "    imdb[\"genre_norm\"] = imdb[\"genre\"].fillna(\"\").astype(str).str.lower().str.strip()\n",
    "    tmdb[\"genre_norm\"] = tmdb[\"genre\"].fillna(\"\").astype(str).str.lower().str.strip()\n",
    "\n",
    "    return imdb, tmdb\n",
    "\n",
    "def exact_merge(imdb, tmdb):\n",
    "    \"\"\"\n",
    "    Exact matching on normalized title + release_year\n",
    "    Keep all imdb rows (left join), bring tmdb columns.\n",
    "    Ensures no duplicate columns appear in the merge.\n",
    "    \"\"\"\n",
    "    left = imdb.copy()\n",
    "    right = tmdb.copy()\n",
    "\n",
    "    # Columns to bring from TMDB (exclude title, title_norm, genre_norm, release_year to avoid duplicates)\n",
    "    tmdb_cols_to_add = [\n",
    "        c for c in right.columns \n",
    "        if c not in [\"title\", \"title_norm\", \"genre_norm\", \"release_year\"]\n",
    "    ]\n",
    "\n",
    "    right_for_merge = right[[\"title_norm\", \"release_year\"] + tmdb_cols_to_add]\n",
    "\n",
    "    # Merge left (IMDb) with right (TMDB)\n",
    "    merged_exact = pd.merge(\n",
    "        left,\n",
    "        right_for_merge,\n",
    "        how=\"left\",\n",
    "        on=[\"title_norm\", \"release_year\"],\n",
    "        suffixes=(\"_imdb\", \"_tmdb\"),\n",
    "        indicator=True\n",
    "    )\n",
    "\n",
    "    return merged_exact\n",
    "\n",
    "def fuzzy_link_remaining(merged_exact, tmdb):\n",
    "    \"\"\"\n",
    "    For rows where _merge == 'left_only', try to fuzzy match with tmdb candidates\n",
    "    Block by release_year when possible to reduce false positives.\n",
    "    Uses recordlinkage when available, otherwise uses rapidfuzz as a fallback.\n",
    "    \"\"\"\n",
    "\n",
    "    left_only = merged_exact[merged_exact[\"_merge\"] == \"left_only\"].copy()\n",
    "    left_only = left_only.reset_index().rename(columns={\"index\": \"imdb_index\"})\n",
    "\n",
    "    # Now I'll Determine the different tmdb rows that matched in exact merge:\n",
    "    \n",
    "    matched_tmdb_rows = merged_exact[merged_exact[\"_merge\"] == \"both\"][\"title_norm\"].astype(str).tolist()\n",
    "    tmdb_candidates = tmdb.copy().reset_index().rename(columns={\"index\": \"tmdb_index\"})\n",
    "\n",
    "    # I'll block by release_year where available and only compare rows with same release_year\n",
    "    matches = [] \n",
    "\n",
    "    if USE_RECORDLINKAGE:\n",
    "        # Use recordlinkage package for a blocked comparison by year\n",
    "        indexer = rl.Index()\n",
    "        indexer.block(left_on=\"release_year\", right_on=\"release_year\")\n",
    "        # Then I will prepare dataframes with a normalized title\n",
    "        left_df = left_only.set_index(\"imdb_index\")[[\"title_norm\", \"release_year\"]]\n",
    "        right_df = tmdb_candidates.set_index(\"tmdb_index\")[[\"title_norm\", \"release_year\"]]\n",
    "        pairs = indexer.index(left_df, right_df)\n",
    "        compare = rl.Compare()\n",
    "        compare.string(\"title_norm\", \"title_norm\", method=\"levenshtein\", threshold=0.80, label=\"title_sim\")\n",
    "        features = compare.compute(pairs, left_df, right_df)\n",
    "        # Now I will filter strong matches\n",
    "        strong = features[features[\"title_sim\"] == 1.0]  # exact by threshold\n",
    "        for (im_i, tm_i) in strong.index:\n",
    "            matches.append((im_i, tm_i, 1.0))\n",
    "        # If none of the matches are exact at the threshold given, you can optionally relax/lower the threshold, this is not implemented here but its something to keep in mind\n",
    "    else:\n",
    "        # Use rapidfuzz `fuzz.ratio` for pairwise scoring for the rows with the same year\n",
    "        from rapidfuzz import fuzz\n",
    "        # Now I will build the index by year for tmdb candidates\n",
    "        tmdb_by_year = {}\n",
    "        for _, r in tmdb_candidates.iterrows():\n",
    "            yr = r[\"release_year\"]\n",
    "            tmdb_by_year.setdefault(int(yr) if not pd.isna(yr) else None, []).append(r)\n",
    "\n",
    "        for _, lm in left_only.iterrows():\n",
    "            yr = lm[\"release_year\"]\n",
    "            candidates = tmdb_by_year.get(int(yr) if not pd.isna(yr) else None, [])\n",
    "            best_score = 0\n",
    "            best_idx = None\n",
    "            for cand in candidates:\n",
    "                score = fuzz.token_sort_ratio(lm[\"title_norm\"], cand[\"title_norm\"]) / 100.0\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_idx = cand[\"tmdb_index\"]\n",
    "            # Now I will accept matches with a score of >= 0.90 to ensure optimal accuracy, this can also be altered if needed\n",
    "            if best_score >= 0.90 and best_idx is not None:\n",
    "                matches.append((lm[\"imdb_index\"], best_idx, best_score))\n",
    "\n",
    "    # Now I'll Build the DataFrame of matches\n",
    "    match_rows = []\n",
    "    for imdb_idx, tmdb_idx, score in matches:\n",
    "        match_rows.append({\"imdb_index\": imdb_idx, \"tmdb_index\": tmdb_idx, \"score\": score})\n",
    "    matches_df = pd.DataFrame(match_rows)\n",
    "\n",
    "    return matches_df\n",
    "\n",
    "#Now I will merge IMDb and TMDB \n",
    "def apply_matches_and_fuse(merged_exact, matches_df, imdb, tmdb):\n",
    "    \"\"\"\n",
    "    Take the exact merge, apply fuzzy matches, and return the final cleaned DataFrame.\n",
    "    Ensures IMDb columns are preserved, TMDB columns are added, and conflicts are handled.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start with a copy of merged_exact\n",
    "    result = merged_exact.copy()\n",
    "    result[\"_merge\"] = result[\"_merge\"].astype(str)\n",
    "\n",
    "    # Then I'll prepare the TMDB map by indexing for an easier lookup\n",
    "    tmdb_map = tmdb.reset_index().rename(columns={\"index\": \"tmdb_index\"}).set_index(\"tmdb_index\")\n",
    "\n",
    "    # Apply fuzzy matches\n",
    "    for _, r in matches_df.iterrows():\n",
    "        imdb_idx = r[\"imdb_index\"]\n",
    "        tmdb_idx = r[\"tmdb_index\"]\n",
    "\n",
    "        # This code will then find rows in the result where index matches imdb_idx\n",
    "        mask = result.index == imdb_idx\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # Fill in TMDB columns only where missing\n",
    "        for col in tmdb.columns:\n",
    "            if col in [\"title\", \"title_norm\", \"genre\", \"genre_norm\", \"release_year\"]:\n",
    "                continue\n",
    "            target_col = col if col in result.columns else col + \"_tmdb\"\n",
    "            val = tmdb_map.loc[tmdb_idx].get(col)\n",
    "            if target_col in result.columns:\n",
    "                if pd.isna(result.loc[imdb_idx, target_col]):\n",
    "                    result.loc[imdb_idx, target_col] = val\n",
    "            else:\n",
    "                result.loc[imdb_idx, target_col] = val\n",
    "\n",
    "        result.loc[imdb_idx, \"_merge\"] = \"fuzzy\"\n",
    "\n",
    "    # This cleans the runtime columns\n",
    "    def clean_runtime(col):\n",
    "        if col.dtype == \"string\" or col.dtype == \"object\":\n",
    "            # Extract numeric part from strings like \"142 min\"\n",
    "            return pd.to_numeric(col.str.extract(r\"(\\d+)\")[0], errors=\"coerce\").astype(\"Int64\")\n",
    "        else:\n",
    "            return col.astype(\"Int64\")\n",
    "\n",
    "    # Now I will construct the final DataFrame with clear column names\n",
    "    final = pd.DataFrame(index=result.index)\n",
    "\n",
    "    # Titles and year\n",
    "    final[\"title\"] = result[\"title\"]\n",
    "    final[\"release_year\"] = result[\"release_year\"]\n",
    "    final[\"director\"] = result.get(\"director\", pd.Series(index=result.index, dtype=\"string\"))\n",
    "\n",
    "    # Genres: preserve IMDb and TMDB separately\n",
    "    final[\"genre_imdb\"] = result.get(\"genre\", result.get(\"genre_imdb\", pd.Series(index=result.index, dtype=\"string\")))\n",
    "    final[\"genre_tmdb\"] = result.get(\"genre_tmdb\", result.get(\"genre_tmdb\", pd.Series(index=result.index, dtype=\"string\")))\n",
    "\n",
    "    # Ratings\n",
    "    final[\"rating_imdb\"] = result.get(\"rating\", result.get(\"rating_imdb\", pd.Series(index=result.index, dtype=\"float\")))\n",
    "    final[\"vote_average_tmdb\"] = result.get(\"vote_average\", result.get(\"vote_average_tmdb\", pd.Series(index=result.index, dtype=\"float\")))\n",
    "    final[\"vote_count_tmdb\"] = result.get(\"vote_count\", result.get(\"vote_count_tmdb\", pd.Series(index=result.index, dtype=\"Int64\")))\n",
    "\n",
    "    # Financials\n",
    "    final[\"budget_in_millions\"] = result.get(\"budget_in_millions\", pd.Series(index=result.index, dtype=\"float\"))\n",
    "    final[\"revenue_in_millions\"] = result.get(\"revenue_in_millions\", pd.Series(index=result.index, dtype=\"float\"))\n",
    "    final[\"gross_in_millions\"] = result.get(\"gross_in_millions\", pd.Series(index=result.index, dtype=\"float\"))\n",
    "    final[\"popularity\"] = result.get(\"popularity\", pd.Series(index=result.index, dtype=\"float\"))\n",
    "\n",
    "    # Runtimes:\n",
    "    #IMDb runtime\n",
    "    imdb_runtime_col = None\n",
    "    for col in [\"runtime_in_minutes\", \"runtime_in_minutes_imdb\", \"runtime_imdb\"]:\n",
    "        if col in result.columns:\n",
    "            imdb_runtime_col = col\n",
    "            break\n",
    "    final[\"runtime_imdb\"] = (\n",
    "        clean_runtime(result[imdb_runtime_col])\n",
    "        if imdb_runtime_col is not None\n",
    "        else pd.Series(index=result.index, dtype=\"Int64\")\n",
    "    )\n",
    "\n",
    "    # TMDB runtime\n",
    "    tmdb_runtime_col = None\n",
    "    for col in [\"runtime_in_minutes_tmdb\", \"runtime_tmdb\"]:\n",
    "        if col in result.columns:\n",
    "            tmdb_runtime_col = col\n",
    "            break\n",
    "    final[\"runtime_tmdb\"] = (\n",
    "        clean_runtime(result[tmdb_runtime_col])\n",
    "        if tmdb_runtime_col is not None\n",
    "        else pd.Series(index=result.index, dtype=\"Int64\")\n",
    "    )\n",
    "\n",
    "    # Metascore\n",
    "    final[\"metascore\"] = result.get(\"metascore\", result.get(\"metascore_imdb\", pd.Series(index=result.index, dtype=\"float\")))\n",
    "\n",
    "    # Merge provenance\n",
    "    final[\"_merge_status\"] = result[\"_merge\"]\n",
    "\n",
    "    # Reorder columns for readability\n",
    "    cols = [\n",
    "        \"title\", \"release_year\", \"director\",\n",
    "        \"genre_imdb\", \"genre_tmdb\",\n",
    "        \"rating_imdb\", \"vote_average_tmdb\", \"vote_count_tmdb\",\n",
    "        \"budget_in_millions\", \"revenue_in_millions\", \"gross_in_millions\", \"popularity\",\n",
    "        \"runtime_imdb\", \"runtime_tmdb\",\n",
    "        \"metascore\", \"_merge_status\"\n",
    "    ]\n",
    "    final = final[cols]\n",
    "\n",
    "    return final\n",
    "              \n",
    "    \n",
    "def make_unique_cols(df):\n",
    "    \"\"\"\n",
    "    Ensure all column names in the DataFrame are unique by appending _1, _2, etc. \n",
    "    to duplicates (after the first occurrence).\n",
    "    \"\"\"\n",
    "    cols = pd.Series(df.columns)\n",
    "    for dup in cols[cols.duplicated()].unique():\n",
    "        dup_idx = cols[cols == dup].index.values\n",
    "        cols[dup_idx[1:]] = [f\"{dup}_{i}\" for i in range(1, len(dup_idx))]\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    imdb, tmdb = load_and_preview()\n",
    "    imdb = make_unique_cols(imdb)\n",
    "    tmdb = make_unique_cols(tmdb)\n",
    "    imdb, tmdb = normalize_columns(imdb, tmdb)\n",
    "\n",
    "    # Here are some Basic schema prints for our discretion and documentation of the project\n",
    "    print(\"IMDB columns:\", imdb.columns.tolist())\n",
    "    print(\"TMDB columns:\", tmdb.columns.tolist())\n",
    "\n",
    "    imdb = imdb.loc[:, ~imdb.columns.duplicated()]\n",
    "    tmdb = tmdb.loc[:, ~tmdb.columns.duplicated()]\n",
    "    \n",
    "    merged_exact = exact_merge(imdb, tmdb)\n",
    "\n",
    "    counts = {\n",
    "        \"imdb_total_rows\": len(imdb),\n",
    "        \"tmdb_total_rows\": len(tmdb),\n",
    "        \"exact_match_count\": int((merged_exact[\"_merge\"] == \"both\").sum()),\n",
    "        \"left_only_before_fuzzy\": int((merged_exact[\"_merge\"] == \"left_only\").sum())\n",
    "    }\n",
    "\n",
    "    # Now I will run fuzzy linkage to match remaining left_only rows\n",
    "    matches_df = fuzzy_link_remaining(merged_exact, tmdb)\n",
    "\n",
    "    counts[\"fuzzy_match_count\"] = int(len(matches_df))\n",
    "\n",
    "    merged_final = apply_matches_and_fuse(merged_exact, matches_df, imdb, tmdb)\n",
    "\n",
    "    # Finally, I will save the results to use for analysis\n",
    "    merged_final.to_csv(MERGED_CSV, index=False)\n",
    "\n",
    "    counts[\"final_merged_rows\"] = len(merged_final)\n",
    "    counts[\"exact_matches_saved\"] = int((merged_final[\"_merge_status\"] == \"both\").sum())\n",
    "    counts[\"fuzzy_matches_saved\"] = int((merged_final[\"_merge_status\"] == \"fuzzy\").sum())\n",
    "    counts[\"unmatched_imdb_saved\"] = int((merged_final[\"_merge_status\"] == \"left_only\").sum())\n",
    "\n",
    "    with open(LOG_JSON, \"w\") as f:\n",
    "        json.dump(counts, f, indent=2)\n",
    "\n",
    "    print(\"Integration done. Outputs:\")\n",
    "    print(\" -\", MERGED_CSV)\n",
    "    print(\" -\", LOG_JSON)\n",
    "    print(json.dumps(counts, indent=2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
