{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "961bb6bf-a6d1-4c62-b41d-dfa7b91958eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting recordlinkage\n",
      "  Downloading recordlinkage-0.16-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting rapidfuzz\n",
      "  Downloading rapidfuzz-3.14.3-cp312-cp312-macosx_10_13_x86_64.whl.metadata (12 kB)\n",
      "Collecting jellyfish>=1 (from recordlinkage)\n",
      "  Downloading jellyfish-1.2.1-cp312-cp312-macosx_10_12_x86_64.whl.metadata (642 bytes)\n",
      "Requirement already satisfied: numpy>=1.13 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from recordlinkage) (1.26.4)\n",
      "Requirement already satisfied: pandas<3,>=1 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from recordlinkage) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from recordlinkage) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from recordlinkage) (1.5.1)\n",
      "Requirement already satisfied: joblib in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from recordlinkage) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from pandas<3,>=1->recordlinkage) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from pandas<3,>=1->recordlinkage) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from pandas<3,>=1->recordlinkage) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from scikit-learn>=1->recordlinkage) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/briannamarroquin/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1->recordlinkage) (1.16.0)\n",
      "Downloading recordlinkage-0.16-py3-none-any.whl (926 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.9/926.9 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.14.3-cp312-cp312-macosx_10_13_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jellyfish-1.2.1-cp312-cp312-macosx_10_12_x86_64.whl (323 kB)\n",
      "Installing collected packages: rapidfuzz, jellyfish, recordlinkage\n",
      "Successfully installed jellyfish-1.2.1 rapidfuzz-3.14.3 recordlinkage-0.16\n"
     ]
    }
   ],
   "source": [
    "!pip install recordlinkage rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e91a685-8017-40c8-8915-397f163dcb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB columns: ['title', 'director', 'release_year', 'genre', 'rating', 'metascore', 'runtime_in_minutes', 'gross_in_millions', 'title_norm', 'genre_norm']\n",
      "TMDB columns: ['budget_in_millions', 'popularity', 'revenue_in_millions', 'runtime_in_minutes', 'title', 'vote_average', 'vote_count', 'genre', 'release_year', 'title_norm', 'genre_norm']\n",
      "Integration done. Outputs:\n",
      " - integration_output/merged_movies.csv\n",
      " - integration_output/merge_log.json\n",
      "{\n",
      "  \"imdb_total_rows\": 705,\n",
      "  \"tmdb_total_rows\": 3177,\n",
      "  \"exact_match_count\": 379,\n",
      "  \"left_only_before_fuzzy\": 326,\n",
      "  \"fuzzy_match_count\": 8,\n",
      "  \"final_merged_rows\": 705,\n",
      "  \"exact_matches_saved\": 379,\n",
      "  \"fuzzy_matches_saved\": 8,\n",
      "  \"unmatched_imdb_saved\": 318\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# First I will try to import recordlinkage, if not, it the funtion should fallback to rapidfuzz\n",
    "USE_RECORDLINKAGE = False\n",
    "try:\n",
    "    import recordlinkage as rl\n",
    "    USE_RECORDLINKAGE = True\n",
    "except Exception:\n",
    "    try:\n",
    "        from rapidfuzz import fuzz, process\n",
    "    except Exception:\n",
    "        raise RuntimeError(\"Please install either 'recordlinkage' or 'rapidfuzz' to run fuzzy linking.\")\n",
    "\n",
    "IMDB_PATH = \"imdb_cleaned.csv\"\n",
    "TMDB_PATH = \"tmdb_cleaned.csv\"\n",
    "\n",
    "OUTPUT_DIR = \"integration_output\"\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "MERGED_CSV = Path(OUTPUT_DIR) / \"merged_movies.csv\"\n",
    "LOG_JSON = Path(OUTPUT_DIR) / \"merge_log.json\"\n",
    "\n",
    "# These will be the Mapping rules (which columns to keep & which hold precedence/priority over other columns)\n",
    "# I will keep all imdb columns, and add tmdb-specific columns that imdb does not have to aid in our project endeavors.\n",
    "IMDB_KEEP = [\"title\", \"director\", \"release_year\", \"genre\", \"rating\", \"metascore\", \"runtime_in_minutes\", \"gross_in_millions\"]\n",
    "TMDB_KEEP = [\"budget_in_millions\", \"popularity\", \"revenue_in_millions\", \"runtime_in_minutes\", \"vote_average\", \"vote_count\", \"genre\", \"release_year\"]\n",
    "\n",
    "# Precedence rules I will enforce in the python code below:\n",
    "# If imdb and tmdb both provide a value for a shared column:\n",
    "# - For ratings: prefer IMDb 'rating' as they are generally used more often, and also preserve TMDB 'vote_average' as separate column\n",
    "# - For runtime/release_year/genre: if they disagree, keep both columns (runtime_imdb, runtime_tmdb) so downstream analysis can decide (I may also use the average of both for a better representation, during analysis i will revisit this)\n",
    "# - For budget/revenue/popularity prefer TMDB (IMDb often lacks budget/revenue for older movies, TMDB is more up-to-date)\n",
    "# - Title matching uses normalized title + release_year. If release_year missing, block less strictly but prefer exact title.\n",
    "\n",
    "def load_and_preview(imdb_path=IMDB_PATH, tmdb_path=TMDB_PATH):\n",
    "    imdb = pd.read_csv(imdb_path).convert_dtypes()\n",
    "    tmdb = pd.read_csv(tmdb_path).convert_dtypes()\n",
    "\n",
    "    # Remove any duplicate columns, this ensures a clean schema before merging, as I ran into issues when columns were named the same\n",
    "    imdb = imdb.loc[:, ~imdb.columns.duplicated()]\n",
    "    tmdb = tmdb.loc[:, ~tmdb.columns.duplicated()]\n",
    "\n",
    "    return imdb, tmdb\n",
    "\n",
    "def normalize_columns(imdb, tmdb):\n",
    "    imdb = imdb.rename(columns=lambda c: c.strip())\n",
    "    tmdb = tmdb.rename(columns=lambda c: c.strip())\n",
    "\n",
    "    imdb = imdb.loc[:, ~imdb.columns.duplicated()]\n",
    "    tmdb = tmdb.loc[:, ~tmdb.columns.duplicated()]\n",
    "\n",
    "    imdb = make_unique_cols(imdb)\n",
    "    tmdb = make_unique_cols(tmdb)\n",
    "\n",
    "    # Standardize column names (relitively simple since my cleaning addressed these)\n",
    "    imdb = imdb.rename(columns={\n",
    "        \"title\": \"title\",\n",
    "        \"director\": \"director\",\n",
    "        \"release_year\": \"release_year\",\n",
    "        \"genre\": \"genre\",\n",
    "        \"rating\": \"rating\",\n",
    "        \"metascore\": \"metascore\",\n",
    "        \"runtime_in_minutes\": \"runtime_in_minutes\",\n",
    "        \"gross_in_millions\": \"gross_in_millions\"\n",
    "    })\n",
    "\n",
    "    tmdb = tmdb.rename(columns={\n",
    "        \"budget_in_millions\": \"budget_in_millions\",\n",
    "        \"popularity\": \"popularity\",\n",
    "        \"revenue_in_millions\": \"revenue_in_millions\",\n",
    "        \"runtime_in_minutes\": \"runtime_in_minutes\",\n",
    "        \"title\": \"title\",\n",
    "        \"vote_average\": \"vote_average\",\n",
    "        \"vote_count\": \"vote_count\",\n",
    "        \"genre\": \"genre\",\n",
    "        \"release_year\": \"release_year\"\n",
    "    })\n",
    "\n",
    "    # Normalize types: ensure numeric columns are numeric and ensure years are read in as integers\n",
    "    for col in [\"release_year\", \"runtime_in_minutes\"]:\n",
    "        if col in imdb.columns:\n",
    "            imdb[col] = pd.to_numeric(imdb[col], errors=\"coerce\").astype(\"Int64\")\n",
    "        if col in tmdb.columns:\n",
    "            tmdb[col] = pd.to_numeric(tmdb[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    for col in [\"rating\", \"metascore\", \"gross_in_millions\", \"budget_in_millions\", \"popularity\", \"revenue_in_millions\", \"vote_average\", \"vote_count\"]:\n",
    "        if col in imdb.columns:\n",
    "            imdb[col] = pd.to_numeric(imdb[col], errors=\"coerce\")\n",
    "        if col in tmdb.columns:\n",
    "            tmdb[col] = pd.to_numeric(tmdb[col], errors=\"coerce\")\n",
    "\n",
    "    # Here I Normalize title strings by using strip, lower, and remove extra whitespace\n",
    "    def norm_title(s):\n",
    "        if pd.isna(s):\n",
    "            return \"\"\n",
    "        return \" \".join(str(s).strip().lower().split())\n",
    "\n",
    "    imdb[\"title_norm\"] = imdb[\"title\"].apply(norm_title)\n",
    "    tmdb[\"title_norm\"] = tmdb[\"title\"].apply(norm_title)\n",
    "\n",
    "    # Now I'll also create genre_norm for matching (lower & strip)\n",
    "    imdb[\"genre_norm\"] = imdb[\"genre\"].fillna(\"\").astype(str).str.lower().str.strip()\n",
    "    tmdb[\"genre_norm\"] = tmdb[\"genre\"].fillna(\"\").astype(str).str.lower().str.strip()\n",
    "\n",
    "    return imdb, tmdb\n",
    "\n",
    "def exact_merge(imdb, tmdb):\n",
    "    \"\"\"\n",
    "    Exact matching on normalized title + release_year\n",
    "    Keep all imdb rows (left join), bring tmdb columns.\n",
    "    Ensures no duplicate columns appear in the merge.\n",
    "    \"\"\"\n",
    "    left = imdb.copy()\n",
    "    right = tmdb.copy()\n",
    "\n",
    "    # Columns to bring from TMDB (exclude title, title_norm, genre_norm, release_year to avoid duplicates)\n",
    "    tmdb_cols_to_add = [\n",
    "        c for c in right.columns \n",
    "        if c not in [\"title\", \"title_norm\", \"genre_norm\", \"release_year\"]\n",
    "    ]\n",
    "\n",
    "    right_for_merge = right[[\"title_norm\", \"release_year\"] + tmdb_cols_to_add]\n",
    "\n",
    "    # Merge left (IMDb) with right (TMDB)\n",
    "    merged_exact = pd.merge(\n",
    "        left,\n",
    "        right_for_merge,\n",
    "        how=\"left\",\n",
    "        on=[\"title_norm\", \"release_year\"],\n",
    "        suffixes=(\"_imdb\", \"_tmdb\"),\n",
    "        indicator=True\n",
    "    )\n",
    "\n",
    "    return merged_exact\n",
    "\n",
    "def fuzzy_link_remaining(merged_exact, tmdb):\n",
    "    \"\"\"\n",
    "    For rows where _merge == 'left_only', try to fuzzy match with tmdb candidates\n",
    "    Block by release_year when possible to reduce false positives.\n",
    "    Uses recordlinkage if available; otherwise uses rapidfuzz as fallback.\n",
    "    \"\"\"\n",
    "\n",
    "    left_only = merged_exact[merged_exact[\"_merge\"] == \"left_only\"].copy()\n",
    "    left_only = left_only.reset_index().rename(columns={\"index\": \"imdb_index\"})\n",
    "\n",
    "    # Candidate TMDB set is all tmdb rows not already used by exact merge matches\n",
    "    # Now I'll Determine the different tmdb rows that matched in exact merge:\n",
    "    \n",
    "    matched_tmdb_rows = merged_exact[merged_exact[\"_merge\"] == \"both\"][\"title_norm\"].astype(str).tolist()\n",
    "    tmdb_candidates = tmdb.copy().reset_index().rename(columns={\"index\": \"tmdb_index\"})\n",
    "\n",
    "    # I'll block by release_year where available and only compare rows with same release_year\n",
    "    matches = [] \n",
    "\n",
    "    if USE_RECORDLINKAGE:\n",
    "        # Use recordlinkage package for a blocked comparison by year\n",
    "        indexer = rl.Index()\n",
    "        indexer.block(left_on=\"release_year\", right_on=\"release_year\")\n",
    "        # Then I will prepare dataframes with a normalized title\n",
    "        left_df = left_only.set_index(\"imdb_index\")[[\"title_norm\", \"release_year\"]]\n",
    "        right_df = tmdb_candidates.set_index(\"tmdb_index\")[[\"title_norm\", \"release_year\"]]\n",
    "        pairs = indexer.index(left_df, right_df)\n",
    "        compare = rl.Compare()\n",
    "        compare.string(\"title_norm\", \"title_norm\", method=\"levenshtein\", threshold=0.80, label=\"title_sim\")\n",
    "        features = compare.compute(pairs, left_df, right_df)\n",
    "        # Now I will filter strong matches\n",
    "        strong = features[features[\"title_sim\"] == 1.0]  # exact by threshold\n",
    "        for (im_i, tm_i) in strong.index:\n",
    "            matches.append((im_i, tm_i, 1.0))\n",
    "        # If none of the matches are exact at the threshold given, you can optionally relax/lower the threshold, this is not implemented here but its something to keep in mind\n",
    "    else:\n",
    "        # Use rapidfuzz `fuzz.ratio` for pairwise scoring for the rows with the same year\n",
    "        from rapidfuzz import fuzz\n",
    "        # Now I will build the index by year for tmdb candidates\n",
    "        tmdb_by_year = {}\n",
    "        for _, r in tmdb_candidates.iterrows():\n",
    "            yr = r[\"release_year\"]\n",
    "            tmdb_by_year.setdefault(int(yr) if not pd.isna(yr) else None, []).append(r)\n",
    "\n",
    "        for _, lm in left_only.iterrows():\n",
    "            yr = lm[\"release_year\"]\n",
    "            candidates = tmdb_by_year.get(int(yr) if not pd.isna(yr) else None, [])\n",
    "            best_score = 0\n",
    "            best_idx = None\n",
    "            for cand in candidates:\n",
    "                score = fuzz.token_sort_ratio(lm[\"title_norm\"], cand[\"title_norm\"]) / 100.0\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_idx = cand[\"tmdb_index\"]\n",
    "            # Now I will accept matches with a score of >= 0.90 to ensure optimal accuracy, this can also be altered if needed\n",
    "            if best_score >= 0.90 and best_idx is not None:\n",
    "                matches.append((lm[\"imdb_index\"], best_idx, best_score))\n",
    "\n",
    "    # Now I'll Build the DataFrame of matches\n",
    "    match_rows = []\n",
    "    for imdb_idx, tmdb_idx, score in matches:\n",
    "        match_rows.append({\"imdb_index\": imdb_idx, \"tmdb_index\": tmdb_idx, \"score\": score})\n",
    "    matches_df = pd.DataFrame(match_rows)\n",
    "\n",
    "    return matches_df\n",
    "\n",
    "def apply_matches_and_fuse(merged_exact, matches_df, imdb, tmdb):\n",
    "    # merged_exact currently contains left imdb rows, and _merge indicator. We'll add fuzzy matches there to ensure there are matches in the data, even if they arent as strong\n",
    "    result = merged_exact.copy()\n",
    "\n",
    "    result[\"_merge\"] = result[\"_merge\"].astype(str)\n",
    "\n",
    "    # Now, I'll prepare the tmdb map by index\n",
    "    tmdb_map = tmdb.reset_index().rename(columns={\"index\": \"tmdb_index\"}).set_index(\"tmdb_index\")\n",
    "\n",
    "    # For each fuzzy match, the below code will fill missing tmdb columns into the result row by matching the imdb index\n",
    "    for _, r in matches_df.iterrows():\n",
    "        imdb_idx = r[\"imdb_index\"]\n",
    "        tmdb_idx = r[\"tmdb_index\"]\n",
    "        score = r[\"score\"]\n",
    "\n",
    "        # Find row in result where original row index == imdb_idx\n",
    "        mask = result.index == imdb_idx\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # For columns coming from tmdb, I will fill in a variable when merged_exact has NaN/Na for the column\n",
    "        for col in tmdb.columns:\n",
    "            if col in [\"title\", \"title_norm\", \"genre\", \"genre_norm\"]:\n",
    "                continue\n",
    "            tm_col = col\n",
    "            target_col = tm_col if tm_col in result.columns else tm_col + \"_tmdb\"\n",
    "            val = tmdb_map.loc[tmdb_idx].get(tm_col)\n",
    "            if target_col in result.columns:\n",
    "                if pd.isna(result.loc[imdb_idx, target_col]):\n",
    "                    result.loc[imdb_idx, target_col] = val\n",
    "            else:\n",
    "                result.loc[imdb_idx, target_col] = val\n",
    "\n",
    "        result.loc[imdb_idx, \"_merge\"] = \"fuzzy\"\n",
    "\n",
    "    # Now we can standardize ou output column set and resolve any conflicts that arise\n",
    "    # I'll create final columns with clear names to avoid any confusion:\n",
    "    # Title, release_year, director (from imdb), genre_imdb, genre_tmdb, rating_imdb, vote_average_tmdb, budget_in_millions, revenue_in_millions, popularity, vote_count, runtime_imdb, runtime_tmdb, metascore, gross_in_millions\n",
    "    final = pd.DataFrame()\n",
    "    final[\"title\"] = result[\"title\"]\n",
    "    final[\"release_year\"] = result[\"release_year\"]\n",
    "\n",
    "    final[\"director\"] = result.get(\"director\", pd.Series(index=result.index, dtype=\"string\"))\n",
    "    final[\"genre_imdb\"] = result.get(\"genre\", pd.Series(index=result.index, dtype=\"string\"))\n",
    "    final[\"genre_tmdb\"] = result.get(\"genre_tmdb\", result.get(\"genre_tmdb\", pd.Series(index=result.index, dtype=\"string\")))\n",
    "\n",
    "    final[\"rating_imdb\"] = result.get(\"rating\", pd.Series(index=result.index, dtype=\"float\"))\n",
    "    final[\"vote_average_tmdb\"] = result.get(\"vote_average\", result.get(\"vote_average_tmdb\", pd.Series(index=result.index, dtype=\"float\")))\n",
    "    final[\"vote_count_tmdb\"] = result.get(\"vote_count\", result.get(\"vote_count_tmdb\", pd.Series(index=result.index, dtype=\"Int64\")))\n",
    "\n",
    "    # Final numbers\n",
    "    final[\"gross_in_millions\"] = result.get(\"gross_in_millions\", pd.Series(index=result.index, dtype=\"float\"))\n",
    "    final[\"budget_in_millions\"] = result.get(\"budget_in_millions\", pd.Series(index=result.index, dtype=\"float\"))\n",
    "    final[\"revenue_in_millions\"] = result.get(\"revenue_in_millions\", pd.Series(index=result.index, dtype=\"float\"))\n",
    "    final[\"popularity\"] = result.get(\"popularity\", pd.Series(index=result.index, dtype=\"float\"))\n",
    "\n",
    "    # Final runtimes\n",
    "    final[\"runtime_imdb\"] = result.get(\"runtime_in_minutes\", pd.Series(index=result.index, dtype=\"Int64\"))\n",
    "    final[\"runtime_tmdb\"] = result.get(\"runtime_in_minutes_tmdb\", result.get(\"runtime_in_minutes_tmdb\", pd.Series(index=result.index, dtype=\"Int64\")))\n",
    "\n",
    "    # Final metascore\n",
    "    final[\"metascore\"] = result.get(\"metascore\", pd.Series(index=result.index, dtype=\"float\"))\n",
    "\n",
    "    # Final provenance\n",
    "    final[\"_merge_status\"] = result[\"_merge\"]\n",
    "\n",
    "    # Here, I reposition columns for readability and also clarity \n",
    "    cols = [\n",
    "        \"title\", \"release_year\", \"director\",\n",
    "        \"genre_imdb\", \"genre_tmdb\",\n",
    "        \"rating_imdb\", \"vote_average_tmdb\", \"vote_count_tmdb\",\n",
    "        \"budget_in_millions\", \"revenue_in_millions\", \"gross_in_millions\", \"popularity\",\n",
    "        \"runtime_imdb\", \"runtime_tmdb\",\n",
    "        \"metascore\", \"_merge_status\"\n",
    "    ]\n",
    "    final = final[cols]\n",
    "\n",
    "    return final\n",
    "    \n",
    "def make_unique_cols(df):\n",
    "    \"\"\"\n",
    "    Ensure all column names in the DataFrame are unique by appending _1, _2, etc. \n",
    "    to duplicates (after the first occurrence).\n",
    "    \"\"\"\n",
    "    cols = pd.Series(df.columns)\n",
    "    for dup in cols[cols.duplicated()].unique():\n",
    "        dup_idx = cols[cols == dup].index.values\n",
    "        cols[dup_idx[1:]] = [f\"{dup}_{i}\" for i in range(1, len(dup_idx))]\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    imdb, tmdb = load_and_preview()\n",
    "    imdb = make_unique_cols(imdb)\n",
    "    tmdb = make_unique_cols(tmdb)\n",
    "    imdb, tmdb = normalize_columns(imdb, tmdb)\n",
    "\n",
    "    # Here are some Basic schema prints for our discretion and documentation of the project\n",
    "    print(\"IMDB columns:\", imdb.columns.tolist())\n",
    "    print(\"TMDB columns:\", tmdb.columns.tolist())\n",
    "\n",
    "    imdb = imdb.loc[:, ~imdb.columns.duplicated()]\n",
    "    tmdb = tmdb.loc[:, ~tmdb.columns.duplicated()]\n",
    "    \n",
    "    merged_exact = exact_merge(imdb, tmdb)\n",
    "\n",
    "    counts = {\n",
    "        \"imdb_total_rows\": len(imdb),\n",
    "        \"tmdb_total_rows\": len(tmdb),\n",
    "        \"exact_match_count\": int((merged_exact[\"_merge\"] == \"both\").sum()),\n",
    "        \"left_only_before_fuzzy\": int((merged_exact[\"_merge\"] == \"left_only\").sum())\n",
    "    }\n",
    "\n",
    "    # Now I will run fuzzy linkage to match remaining left_only rows\n",
    "    matches_df = fuzzy_link_remaining(merged_exact, tmdb)\n",
    "\n",
    "    counts[\"fuzzy_match_count\"] = int(len(matches_df))\n",
    "\n",
    "    merged_final = apply_matches_and_fuse(merged_exact, matches_df, imdb, tmdb)\n",
    "\n",
    "    # Finally, I will save the results to use for analysis\n",
    "    merged_final.to_csv(MERGED_CSV, index=False)\n",
    "\n",
    "    counts[\"final_merged_rows\"] = len(merged_final)\n",
    "    counts[\"exact_matches_saved\"] = int((merged_final[\"_merge_status\"] == \"both\").sum())\n",
    "    counts[\"fuzzy_matches_saved\"] = int((merged_final[\"_merge_status\"] == \"fuzzy\").sum())\n",
    "    counts[\"unmatched_imdb_saved\"] = int((merged_final[\"_merge_status\"] == \"left_only\").sum())\n",
    "\n",
    "    with open(LOG_JSON, \"w\") as f:\n",
    "        json.dump(counts, f, indent=2)\n",
    "\n",
    "    print(\"Integration done. Outputs:\")\n",
    "    print(\" -\", MERGED_CSV)\n",
    "    print(\" -\", LOG_JSON)\n",
    "    print(json.dumps(counts, indent=2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
